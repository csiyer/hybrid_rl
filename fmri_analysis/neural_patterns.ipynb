{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from nilearn import masking\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import neural_helpers as nh\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "nibs_base = '/Volumes/shohamy-locker/chris/hybrid_mri_bids/derivatives/nibetaseries'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract & normalize hippocampal patterns\n",
    "(smooth? ~2mm kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hipp_mask = nib.load('/path/to/hippocampus_mask.nii.gz')  # Bilateral hippocampal mask\n",
    "\n",
    "# extract all hippocampal patterns (trials x voxels x subjects)\n",
    "all_hipp_patterns = []\n",
    "for sub in [i for i in sorted(os.listdir(nibs_base)) if 'sub-hybrid' in i]:\n",
    "    sub_hipp_patterns = []\n",
    "    sub_beta_files = glob.glob(nibs_base+f'/{sub}/beta_*.nii.gz')\n",
    "    for trial_beta_file in sub_beta_files:\n",
    "        beta_img = nib.load(trial_beta_file)\n",
    "        hipp_data = masking.apply_mask(beta_img, hipp_mask)\n",
    "        sub_hipp_patterns.append(hipp_data)\n",
    "\n",
    "    all_hipp_patterns.append(sub_hipp_patterns)\n",
    "\n",
    "# Pattern-level normalization (z-score across voxels per trial)\n",
    "hipp_patterns = np.array(all_hipp_patterns) # (trials x voxels)\n",
    "normalized_patterns = (hipp_patterns - np.mean(hipp_patterns, axis=2, keepdims=True)) / np.std(hipp_patterns, axis=2, keepdims=True)\n",
    "\n",
    "# save\n",
    "pattern_save_path = ''\n",
    "np.save(pattern_save_path, normalized_patterns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beh = pd.read_csv('./data/hybrid_data.csv')\n",
    "# remember to exclude trials from first 6 volumes that don't exist in fmri data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Correlation of encoding to retrieval trials - compare optimal to non-optimal memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding choice trial\n",
    "\n",
    "# encoding feedback trial\n",
    "\n",
    "# does ERS increase with time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Similarity kernel of retrieval trial to those around encoding trial. Does this similarity predict old/new trial performance? old vs. new choices, optimal vs. nonoptimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predicting value\n",
    "- lucky vs. unlucky deck (value matched) on either encoding or retrieval\n",
    "- save vs. different luck state encoding vs. retrieval\n",
    "- predict subsequent retrieval from pattern at feedback of choice image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hbi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
